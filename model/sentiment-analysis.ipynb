{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,<br/>\n",
    "  &ensp;&ensp;&ensp;author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},<br/>\n",
    "  &ensp;&ensp;&ensp;title = {Learning Word Vectors for Sentiment Analysis},<br/>\n",
    "  &ensp;&ensp;&ensp;booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},<br/>\n",
    "  &ensp;&ensp;&ensp;month = {June},<br/>\n",
    "  &ensp;&ensp;&ensp;year = {2011},<br/>\n",
    "  &ensp;&ensp;&ensp;address = {Portland, Oregon, USA},<br/>\n",
    "  &ensp;&ensp;&ensp;publisher = {Association for Computational Linguistics},<br/>\n",
    "  &ensp;&ensp;&ensp;pages = {142--150},<br/>\n",
    "  &ensp;&ensp;&ensp;url = {http://www.aclweb.org/anthology/P11-1015}<br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import timeit\n",
    "\n",
    "# natural language toolkit libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# deep learning libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# files\n",
    "from contractions import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.7.0\n",
      "GPU: AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# check tensorflow version and is gpu available\n",
    "is_available = \"AVAILABLE\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\"\n",
    "print(f\"Version: {tf.__version__}\\nGPU: {is_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to D:\\Portfolio\\nlp-web-app\\model\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 80/80 [06:03<00:00,  4.55s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 1/1 [06:03<00:00, 363.95s/ url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to D:\\Portfolio\\nlp-web-app\\model\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{Split('train'): <PrefetchDataset shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>,\n",
       " Split('test'): <PrefetchDataset shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>,\n",
       " Split('unsupervised'): <PrefetchDataset shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download IMDB dataset\n",
    "tfds.load(name='imdb_reviews', data_dir='[replace with your own data directory]', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 20000, Test Size: 5000, Valid Size: 25000\n"
     ]
    }
   ],
   "source": [
    "# tensorflow load train dataset\n",
    "builder = tfds.core.builder_from_directory('D:\\\\Portfolio\\\\nlp-web-app\\\\model\\\\imdb_reviews\\\\plain_text\\\\1.0.0')\n",
    "\n",
    "# as_supervised argument - set the structure of the dataset as input and label\n",
    "dataset = builder.as_dataset(split=('train[:80%]', 'train[80%:]', 'test'), \n",
    "                             shuffle_files=True, \n",
    "                             as_supervised=True)      # 80% train 20% test\n",
    "train, test, valid = dataset\n",
    "print(f\"Train Size: {len(train)}, Test Size: {len(test)}, Valid Size: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this was an absolutely terrible movie. don't b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been known to fall asleep during films,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mann photographs the alberta rocky mountains i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the kind of film for a snowy sunday af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>as others have mentioned, all the women that g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  this was an absolutely terrible movie. don't b...\n",
       "1      0  i have been known to fall asleep during films,...\n",
       "2      0  mann photographs the alberta rocky mountains i...\n",
       "3      1  this is the kind of film for a snowy sunday af...\n",
       "4      1  as others have mentioned, all the women that g..."
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tfds.as_dataframe(ds=train, ds_info=builder.info)      # require jinja2\n",
    "df = data.copy(deep=True)\n",
    "df['text'] = [x.decode('utf-8').strip().lower() for x in df.text]   # remove trailing and leading whitespace\n",
    "df.head()       # label 0 = negative; 1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://towardsdatascience.com/nlp-learning-series-part-1-text-preprocessing-methods-for-deep-learning-20085601684b\n",
    "# reference: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "# things to consider: drop rows with empty text and spelling corrections\n",
    "def remove_html_tags(text):\n",
    "    if bool(re.search(r'<.*?>', text)):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# expand contractions, e.g., don't -> do not, purpose is to standardize our text\n",
    "def get_contractions(contraction_mapping):\n",
    "    contraction_regex = re.compile('(%s)' % '|'.join(contraction_mapping.keys()))\n",
    "    return contraction_mapping, contraction_regex\n",
    "\n",
    "contractions, contractions_pattern = get_contractions(CONTRACTION_MAP)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_pattern.sub(replace, text)\n",
    "\n",
    "\n",
    "def add_space_between_punctuations(text):\n",
    "    text = re.sub(r'([a-zA-Z])([,.!()])', r'\\1\\2 ', text)       # add space between punctuations and letters\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list = [item for item in stopword_list if item not in ('no', 'not', 'nor', 'any', 'too')]  # useful information\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if not word in stopword_list] \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def remove_punctuations(text, filters):\n",
    "    text = text.translate(str.maketrans('', '', filters))\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_multiple_whitespace(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# main function to clean text - ordering of function can be a factor for data cleaning\n",
    "def clean_text(text, remove_stopword=False, use_bert=False):\n",
    "    text = remove_html_tags(text)\n",
    "    text = add_space_between_punctuations(text)\n",
    "    text = replace_contractions(text)\n",
    "    \n",
    "    if not use_bert:        # allow text in bert (bidirectional encoder representations from transformers)\n",
    "        text = re.sub(r'\\+|\\d+', '', text)\n",
    "\n",
    "    if remove_stopword:\n",
    "        text = remove_stopwords(text)\n",
    "\n",
    "    # reference: https://github.com/hmohebbi/SentimentAnalysis/blob/master/main.ipynb\n",
    "    # save certain punctuations if using bert because bert embeddings was trained on wikipedia\n",
    "    filters = string.punctuation + \"\\t\\n\"\n",
    "    if use_bert:\n",
    "        text = re.sub(r'\\!+', '!', text)\n",
    "        text = re.sub(r'\\!+', '!', text)\n",
    "        filters = set(filters) - set(\"-'!?).;,/:(\")\n",
    "        filters = ''.join(filters)\n",
    "    text = remove_punctuations(text, filters)\n",
    "\n",
    "    if use_bert:        # remove empty brackets\n",
    "        text = re.sub(r'\\( *\\)', ' ', text)\n",
    "\n",
    "    text = remove_multiple_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speeh tagging and wordnet lemmatization\n",
    "# convert penn treebank tag to wordnet tag\n",
    "# reference: https://github.com/prateek22sri/Sentiment-analysis/blob/master/unigramSentiWordNet.py\n",
    "# reference: https://github.com/KT12/tag-lemmatize/blob/master/tag-lemmatize.py\n",
    "# reference: https://wordnet.princeton.edu/documentation/wnintro3wn\n",
    "# other techniques include stemming\n",
    "# stemming is not use in this context as it removes or stems the last few characters, often leading to incorrect spelling\n",
    "def convert_tag(penn_tag):\n",
    "    \"\"\"\n",
    "    Convert between PennTreebank to WordNet tags\n",
    "    \"\"\"\n",
    "    if penn_tag.startswith('N'):     # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif penn_tag.startswith('V'):   # Verb\n",
    "        return wordnet.VERB\n",
    "    elif penn_tag.startswith('J'):   # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif penn_tag.startswith('S'):   # Adjective Satellite\n",
    "        return 's'\n",
    "    elif penn_tag.startswith('R'):   # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None  # other parts of speech will be returned as none\n",
    "\n",
    "def pos_and_lemm(text):       # part-of-speech tagging and word lemmatization\n",
    "    elements = word_tokenize(text)      # tokenize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = nltk.pos_tag(elements)\n",
    "    words = []\n",
    "\n",
    "    # list of tuples [('token'), 'tag'), ('token2'), 'tag2'...]\n",
    "    for word, tag in sentence:\n",
    "        wn_tag = convert_tag(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        words.append(lemmatizer.limmetize(word, wn_tag))\n",
    "    \n",
    "    return ' '.join(words)      # O(n) time complexity, if use += it will be O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this was an absolutely terrible movie. don't b...</td>\n",
       "      <td>this was an absolutely terrible movie do not b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been known to fall asleep during films,...</td>\n",
       "      <td>i have been known to fall asleep during films ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mann photographs the alberta rocky mountains i...</td>\n",
       "      <td>mann photographs the alberta rocky mountains i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the kind of film for a snowy sunday af...</td>\n",
       "      <td>this is the kind of film for a snowy sunday af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>as others have mentioned, all the women that g...</td>\n",
       "      <td>as others have mentioned all the women that go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  this was an absolutely terrible movie. don't b...   \n",
       "1      0  i have been known to fall asleep during films,...   \n",
       "2      0  mann photographs the alberta rocky mountains i...   \n",
       "3      1  this is the kind of film for a snowy sunday af...   \n",
       "4      1  as others have mentioned, all the women that g...   \n",
       "\n",
       "                                               clean  \n",
       "0  this was an absolutely terrible movie do not b...  \n",
       "1  i have been known to fall asleep during films ...  \n",
       "2  mann photographs the alberta rocky mountains i...  \n",
       "3  this is the kind of film for a snowy sunday af...  \n",
       "4  as others have mentioned all the women that go...  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the entire data and separate into new column\n",
    "df['clean'] = df.text.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words - number of words to keep in vocab after tokenization for training the network\n",
    "tokenizer = Tokenizer(num_words=None, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                      lower=True, \n",
    "                      char_level=False,\n",
    "                      document_count=0,\n",
    "                      split=' ')\n",
    "# tokenizer.fit_on_texts(texts)     # texts are normal array\n",
    "# list(tokenizer.word_index.items())[:5]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5766725d235d92a3f587ad84880941c04bca390e8d595877301ef928828de224"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
