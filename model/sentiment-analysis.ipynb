{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,<br/>\n",
    "  &ensp;&ensp;&ensp;author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},<br/>\n",
    "  &ensp;&ensp;&ensp;title = {Learning Word Vectors for Sentiment Analysis},<br/>\n",
    "  &ensp;&ensp;&ensp;booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},<br/>\n",
    "  &ensp;&ensp;&ensp;month = {June},<br/>\n",
    "  &ensp;&ensp;&ensp;year = {2011},<br/>\n",
    "  &ensp;&ensp;&ensp;address = {Portland, Oregon, USA},<br/>\n",
    "  &ensp;&ensp;&ensp;publisher = {Association for Computational Linguistics},<br/>\n",
    "  &ensp;&ensp;&ensp;pages = {142--150},<br/>\n",
    "  &ensp;&ensp;&ensp;url = {http://www.aclweb.org/anthology/P11-1015}<br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dejon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dejon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dejon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dejon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# common libraries\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import tabulate\n",
    "\n",
    "# natural language toolkit libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn.metrics as mtr\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# files\n",
    "from contractions import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.7.0\n",
      "GPU: AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# check tensorflow version and is gpu available\n",
    "is_available = \"AVAILABLE\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\"\n",
    "print(f\"Version: {tf.__version__}\\nGPU: {is_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to D:\\Portfolio\\nlp-web-app\\model\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 80/80 [06:03<00:00,  4.55s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 1/1 [06:03<00:00, 363.95s/ url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to D:\\Portfolio\\nlp-web-app\\model\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{Split('train'): <PrefetchDataset shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>,\n",
       " Split('test'): <PrefetchDataset shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>,\n",
       " Split('unsupervised'): <PrefetchDataset shapes: {label: (), text: ()}, types: {label: tf.int64, text: tf.string}>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download IMDB dataset\n",
    "tfds.load(name='imdb_reviews', data_dir='[replace with your own data directory]', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 20000, Test Size: 5000, Valid Size: 25000\n"
     ]
    }
   ],
   "source": [
    "# tensorflow load train dataset\n",
    "builder = tfds.core.builder_from_directory('D:\\\\Portfolio\\\\nlp-web-app\\\\model\\\\imdb_reviews\\\\plain_text\\\\1.0.0')\n",
    "\n",
    "# as_supervised argument - set the structure of the dataset as input and label\n",
    "dataset = builder.as_dataset(split=('train[:80%]', 'train[80%:]', 'test'), \n",
    "                             shuffle_files=True, \n",
    "                             as_supervised=True)      # 80% train 20% test\n",
    "train, test, valid = dataset\n",
    "print(f\"Train Size: {len(train)}, Test Size: {len(test)}, Valid Size: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tfds.as_dataframe(ds=train, ds_info=builder.info)      # require jinja2\n",
    "test_set = tfds.as_dataframe(ds=test, ds_info=builder.info)\n",
    "\n",
    "# deep copy of dataframe\n",
    "df_train = train_set.copy(deep=True)    # label 0 = negative; 1 = positive\n",
    "df_test = test_set.copy(deep=True)\n",
    "\n",
    "# remove trailing and leading whitespace and lowercase\n",
    "df_train['text'] = [x.decode('utf-8').strip().lower() for x in df_train.text]\n",
    "df_test['text'] = [x.decode('utf-8').strip().lower() for x in df_test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://towardsdatascience.com/nlp-learning-series-part-1-text-preprocessing-methods-for-deep-learning-20085601684b\n",
    "# reference: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "# things to consider: drop rows with empty text and spelling corrections\n",
    "def remove_html_tags(text):\n",
    "    if bool(re.search(r'<.*?>', text)):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# expand contractions, e.g., don't -> do not, purpose is to standardize our text\n",
    "def get_contractions(contraction_mapping):\n",
    "    contraction_regex = re.compile('(%s)' % '|'.join(contraction_mapping.keys()))\n",
    "    return contraction_mapping, contraction_regex\n",
    "\n",
    "contractions, contractions_pattern = get_contractions(CONTRACTION_MAP)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_pattern.sub(replace, text)\n",
    "\n",
    "\n",
    "def add_space_between_punctuations(text):\n",
    "    text = re.sub(r'([a-zA-Z])([,.!()])', r'\\1\\2 ', text)       # add space between punctuations and letters\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list = [item for item in stopword_list if item not in ('no', 'not', 'nor', 'any', 'too')]  # useful information\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if not word in stopword_list] \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def remove_punctuations(text, filters):\n",
    "    text = text.translate(str.maketrans('', '', filters))\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_multiple_whitespace(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# main function to clean text - ordering of function can be a factor for data cleaning\n",
    "def clean_text(text, remove_stopword=False, use_bert=False):\n",
    "    text = remove_html_tags(text)\n",
    "    text = add_space_between_punctuations(text)\n",
    "    text = replace_contractions(text)\n",
    "    \n",
    "    if not use_bert:        # allow text in bert (bidirectional encoder representations from transformers)\n",
    "        text = re.sub(r'\\+|\\d+', '', text)\n",
    "\n",
    "    if remove_stopword:\n",
    "        text = remove_stopwords(text)\n",
    "\n",
    "    # reference: https://github.com/hmohebbi/SentimentAnalysis/blob/master/main.ipynb\n",
    "    # save certain punctuations if using bert because bert embeddings was trained on wikipedia\n",
    "    filters = string.punctuation + \"\\t\\n\"\n",
    "    if use_bert:\n",
    "        text = re.sub(r'\\!+', '!', text)\n",
    "        text = re.sub(r'\\!+', '!', text)\n",
    "        filters = set(filters) - set(\"-'!?).;,/:(\")\n",
    "        filters = ''.join(filters)\n",
    "    text = remove_punctuations(text, filters)\n",
    "\n",
    "    if use_bert:        # remove empty brackets\n",
    "        text = re.sub(r'\\( *\\)', ' ', text)\n",
    "\n",
    "    text = remove_multiple_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speeh tagging and wordnet lemmatization\n",
    "# convert penn treebank tag to wordnet tag\n",
    "# reference: https://github.com/prateek22sri/Sentiment-analysis/blob/master/unigramSentiWordNet.py\n",
    "# reference: https://github.com/KT12/tag-lemmatize/blob/master/tag-lemmatize.py\n",
    "# reference: https://wordnet.princeton.edu/documentation/wnintro3wn\n",
    "# other techniques include stemming\n",
    "# stemming is not use in this context as it removes or stems the last few characters, often leading to incorrect spelling\n",
    "def convert_tag(penn_tag):\n",
    "    \"\"\"\n",
    "    Convert between PennTreebank to WordNet tags\n",
    "    \"\"\"\n",
    "    if penn_tag.startswith('N'):     # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif penn_tag.startswith('V'):   # Verb\n",
    "        return wordnet.VERB\n",
    "    elif penn_tag.startswith('J'):   # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif penn_tag.startswith('S'):   # Adjective Satellite\n",
    "        return 's'\n",
    "    elif penn_tag.startswith('R'):   # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None  # other parts of speech will be returned as none\n",
    "\n",
    "def pos_and_lemm(text):       # part-of-speech tagging and word lemmatization\n",
    "    elements = word_tokenize(text)      # tokenize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = nltk.pos_tag(elements)\n",
    "    words = []\n",
    "\n",
    "    # list of tuples [('token'), 'tag'), ('token2'), 'tag2'...]\n",
    "    for word, tag in sentence:\n",
    "        wn_tag = convert_tag(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "    \n",
    "    return ' '.join(words)      # O(n) time complexity, if use += it will be O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the entire train and test data and separate into new column\n",
    "df_train['clean'] = df_train.text.apply(clean_text)\n",
    "df_test['clean'] = df_test.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize sentences with part of speech tagging\n",
    "df_train['normalized'] = df_train.clean.apply(pos_and_lemm)\n",
    "df_test['normalized'] = df_test.clean.apply(pos_and_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Save Model and Output Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    joblib.dump(model, filename)\n",
    "\n",
    "def print_metrics(accuracy, precision, recall, f1):\n",
    "    metrics_df = np.array([accuracy, precision, recall, f1])\n",
    "    metrics_df = pd.DataFrame([metrics_df], columns=['accuracy', 'precision', 'recall', 'f1'], index=['metrics'])\n",
    "    print('Performance Metrics:\\n')\n",
    "    print(tabulate.tabulate(metrics_df, headers='keys', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes with Stop Words for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_range - a tuple of lower and upper boundary of range of n-values for different n-grams to be extracted\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), use_idf=True)\n",
    "train_feature = vectorizer.fit_transform(df_train['normalized'].ravel())\n",
    "train_sentiment = df_train['label']\n",
    "test_feature = vectorizer.transform(df_test['normalized'].ravel())\n",
    "test_sentiment = df_test['label']\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_feature, train_sentiment)\n",
    "save_model(model=mnb, filename=\"multinomialnb_with_stopwords.joblib\")   # save filename (should check for duplicates)\n",
    "\n",
    "# predict\n",
    "mnb_pred = mnb.predict(test_feature)\n",
    "mnb_prob = mnb.predict_proba(test_feature)[:,1]\n",
    "\n",
    "# print evaluation\n",
    "mnb_accuracy = np.round(mtr.accuracy_score(test_sentiment, mnb_pred), 3)\n",
    "mnb_precision = np.round(mtr.precision_score(test_sentiment, mnb_pred, average=\"macro\"), 3)\n",
    "mnb_recall = np.round(mtr.recall_score(test_sentiment, mnb_pred), 3)\n",
    "mnb_f1 = np.round(mtr.f1_score(test_sentiment, mnb_pred, average=\"macro\"), 3)\n",
    "print_metrics(mnb_accuracy, mnb_precision, mnb_recall, mnb_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words - number of words to keep in vocab after tokenization for training the network\n",
    "tokenizer = Tokenizer(num_words=None, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                      lower=True, \n",
    "                      char_level=False,\n",
    "                      document_count=0,\n",
    "                      split=' ')\n",
    "# tokenizer.fit_on_texts(texts)     # texts are normal array\n",
    "# list(tokenizer.word_index.items())[:5]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5766725d235d92a3f587ad84880941c04bca390e8d595877301ef928828de224"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
